---
title: "Tarea 5"
author:
  - "Juan M Karawacki"
  - "Bruno Pintos"
date: "9-17-2025"
lang: es
output:
  pdf_document: default
  html_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r include=FALSE, warning=FALSE, message=FALSE}
library(bayesrules)
library(tidyverse)
library(janitor)
library(knitr)
library(kableExtra)
```

## Ejercicio 4.19 (Test de Bechdel)

En este ejercicio analizaremos $\pi$, la proporción de películas que
pasan el test de Bechdel, usando los datos de `bechdel`. Para cada
escenario a continuación, especifica el modelo posterior de $\pi$ y
calcula la media y la moda posteriores.

### 1. John tiene una priori plana Beta(1, 1) y analiza películas del año 1980.

```{r include=FALSE}
data(bechdel, package = "bayesrules")

bechdel80 <- bechdel %>% 
              filter(year == 1980)
```

```{r echo = FALSE, fig.width=4, fig.height=3}
plot_beta(alpha = 1, beta = 1)
```

Partimos de que la priori de John es una Beta(1, 1), es decir, una
distribución uniforme sobre el intervalo [0,1]. Esto implica que
inicialmente todos los valores posibles de $\pi$ son igualmente
probables, por lo tanto la priori no aporta información real previa.

```{r include = FALSE}
bechdel80 %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
```

```{r echo = FALSE}
tabla <- data.frame(
  binary = c("FAIL", "PASS", "Total"),
  n = c(10, 4, 14),
  percent = c(0.7142857, 0.2857143, 1.0000000)
)

tabla %>%
  kable(caption = "Películas 1980 – Test de Bechdel",
        col.names = c("Resultado", "Cantidad", "Proporción"),
        digits = 2,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")
```

Al filtrar las películas del año 1980, observamos que de las 14
películas evaluadas, 4 pasaron el test de Bechdel y 10 no lo pasaron.

\newpage

```{r echo = FALSE, fig.width=6, fig.height=3}
plot_beta_binomial(alpha = 1, beta = 1, y = 4, n = 14)
```

Al actualizar la priori con los datos, obtenemos la posterior y dado que
la priori no era informativa, la posterior coincide perfectamente con la
likelihood ya que adopta toda la información que recibe.

```{r include = FALSE}
summarize_beta_binomial(alpha = 1, beta = 1, y = 4, n = 14)
```

En este caso, la distribución posterior es:
$\pi \sim \text{Beta}(5, 11)$, esto lo sabemos ya que, como aprendimos
en la Tarea 4, usando
`summarize_beta_binomial(alpha = 1, beta = 1, y = 4, n = 14)`
automaticamente obtenemos tanto el $\alpha_{\text{post}}$ como
$\beta_{\text{post}}$

```{r echo = FALSE}
tabla2 <- data.frame(
  Modelo = c("Priori", "Posterior"),
  Media = c(0.5000, 0.3125),
  Moda  = c(NA, 0.2857) 
)

tabla2 %>%
  kable(caption = "Resumen: media y moda de pi",
        digits = 3,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")
```

En conclusión, la posterior tiene una media de 0.31 y una moda de 0.29
aproximadamente. Esto sugiere que, según los datos de 1980, la
proporción de películas que pasaron el test de Bechdel fue cercana al
30%, lo cual tiene lógica con lo mencionado en la Tabla 1.

### 2. Al día siguiente, John analiza películas del año 1990, construyendo su análisis a partir del realizado el día anterior.

Ahora filtramos nuevamente las peliculas pero esta vez analizamos las de
los 90.

```{r include=FALSE}
bechdel90 <- bechdel %>% 
              filter(year == 1990)
```

```{r include = FALSE}
bechdel90 %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
```

```{r echo = FALSE}
tabla90 <- data.frame(
  binary = c("FAIL", "PASS", "Total"),
  n = c(9, 6, 15),
  percent = c(0.6, 0.4, 1.0)
)

tabla90 %>%
  kable(caption = "Películas 1990 – Test de Bechdel",
        col.names = c("Resultado", "Cantidad", "Proporción"),
        digits = 2,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")
```

Esta vez, el 40% de las películas aprobaron el test de Bechdel, lo que
representa una evidencia un poco más optimista que la observada en 1980
y dado que John ya actualizó su priori con esos datos, ahora utiliza
como priori la distribución: $\pi \sim \text{Beta}(5, 11)$

```{r echo = FALSE, fig.width=6, fig.height=3}
plot_beta_binomial(alpha = 5, beta = 11, y = 6, n = 15)
```

```{r include = FALSE}
summarize_beta_binomial(alpha = 5, beta = 11, y = 6, n = 15)
```

```{r echo = FALSE}
tabla2_90 <- data.frame(
  Modelo = c("Priori", "Posterior"),
  Media  = c(0.3125, 0.3548),
  Moda   = c(0.2857, 0.3448)
)

tabla2_90 %>%
  kable(caption = "Resumen: media y moda de pi – Películas 1990",
        digits = 3,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")

```

La priori ahora aporta información relevante basada en los datos previos
de 1980, de manera que la posterior se posiciona entre la evidencia
anterior y los nuevos datos de 1990 con distribución
$\pi \sim \text{Beta}(11, 20)$. Este ajuste conservador combina la
información previa con la nueva, reflejándose en un aumento de la media
de $\pi$ de 0.31 a 0.35, mientras que la moda también se desplaza
ligeramente, mostrando cómo la priori influye en la actualización de
nuestra creencia sin exagerar el efecto de los nuevos datos.

### 3. El tercer día, John analiza películas del año 2000, nuevamente acumulando sobre sus análi1is de los dos días previos.

Volvemos a filtrar esta vez para las peliculas del 2000

```{r include=FALSE}
bechdel00 <- bechdel %>% 
              filter(year == 2000)
```

```{r include = FALSE}
bechdel00 %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
```

```{r echo = FALSE}
tabla00 <- data.frame(
  binary = c("FAIL", "PASS", "Total"),
  n = c(34, 29, 63),
  percent = c(0.5397, 0.4603, 1.0)
)

tabla00 %>%
  kable(caption = "Películas 2000 – Test de Bechdel",
        col.names = c("Resultado", "Cantidad", "Proporción"),
        digits = 3,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")
```

Ahora, el 46% de las películas aprobaron el test de Bechdel, la
evidencia más positiva hasta el momento, lo cual tiene sentido, ya que,
dada la forma en que funciona el test, es esperable que en tiempos más
modernos los resultados sean mejores.

```{r echo = FALSE, fig.width=6, fig.height=3}
plot_beta_binomial(alpha = 11, beta = 20, y = 29, n = 63)
```

```{r include = FALSE}
summarize_beta_binomial(alpha = 11, beta = 20, y = 29, n = 63)
```

```{r echo = FALSE}
tabla200 <- data.frame(
  Modelo = c("Priori", "Posterior"),
  Media  = c(0.3548, 0.4255),
  Moda   = c(0.3448, 0.4239)
)

tabla200 %>%
  kable(caption = "Resumen: media y moda de pi – Películas 2000",
        digits = 3,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")

```

La priori refleja la información acumulada de años anteriores, mientras
que la posterior combina esa información con los datos más recientes de
los 2000 , donde $\pi \sim \text{Beta}(40, 54)$. Como resultado, la
media de $\pi$ aumenta a 0.43 y la moda a 0.42, indicando un efecto
positivo consistente con la tendencia de películas más modernas
mostrando una mayor proporción de aprobaciones en el test de Bechdel.

### 4. Jenna, en cambio, comienza su análisis también con una priori Beta(1, 1), pero analiza películas de 1980, 1990 y 2000 todas en el mismo día.

Esta vez analizamos todas las películas de los años anteriores, es
decir, 1980, 1990 y 2000, sumando un total de 92 películas.

```{r include = FALSE}
bechdel8020 <- bechdel %>% 
  filter(year %in% c(1980, 1990, 2000))
```

```{r include = FALSE}
bechdel8020 %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
```

```{r echo = FALSE}
tabla8020 <- data.frame(
  binary = c("FAIL", "PASS", "Total"),
  n = c(53, 39, 92),
  percent = c(0.5761, 0.4239, 1.0)
)

tabla8020 %>%
  kable(caption = "Películas 1980-2000 – Test de Bechdel",
        col.names = c("Resultado", "Cantidad", "Proporción"),
        digits = 3,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")

```

```{r echo = FALSE, fig.width=6, fig.height=3}
plot_beta_binomial(alpha = 1, beta = 1, y = 39, n = 92)
```

```{r include = FALSE}
summarize_beta_binomial(alpha = 1, beta = 1, y = 39, n = 92)
```

```{r echo = FALSE}
tabla28020 <- data.frame(
  Modelo = c("Priori", "Posterior"),
  Media  = c(0.5000, 0.4255),
  Moda   = c(NA, 0.4239)
)

tabla28020 %>%
  kable(caption = "Resumen: media y moda de pi – Películas 1980-2000",
        digits = 3,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")

```

Al analizar juntas las películas de 1980, 1990 y 2000, Jenna parte de
una priori no informativa (Beta(1,1)) y obtiene una posterior con media
de 0.43 y moda de 0.42 que distribuye $\pi \sim \text{Beta}(40, 54)$,
reflejando la evidencia combinada de todas las décadas. Comparando con
John, que fue actualizando su posterior día a día, vemos que ambos
enfoques llevan a la misma distribución y la misma conclusión: la
proporción de películas que pasan el test de Bechdel aumenta con el
tiempo. Sin embargo, mientras la actualización secuencial de John
refleja paso a paso el efecto de cada año, el enfoque de Jenna combina
toda la información de una sola vez, produciendo directamente una
estimación consolidada de la tendencia general a lo largo de las tres
décadas.


\newpage

## Ejercicio 5.7 (Copa del Mundo)

Sea $\lambda$ el número promedio de goles anotados en un partido de la
Copa Mundial Femenina. Analizaremos $\lambda$ mediante el siguiente
modelo Gamma-Poisson, donde los datos $Y_i$ son el número observado de
goles en una muestra de partidos de la Copa del Mundo:

$$
Y_i \mid \lambda \sim \text{Pois}(\lambda), \quad
\lambda \sim \text{Gamma}(1, 0.25)
$$ 

### 1. Grafique y resuma nuestro conocimiento priori sobre $\lambda$

```{r echo = FALSE, fig.width=6, fig.height=3}
plot_gamma(shape = 1, rate = 0.25)
```
```{r include = FALSE}
summarize_gamma(shape = 1, rate = 0.25)
```

```{r echo = FALSE, fig.width=6, fig.height=3}
tabla_prior <- data.frame(
  Media = 4,
  Moda  = 0,
  Varianza = 16,
  Desviacion = 4
)

tabla_prior %>%
  kable(caption = "Resumen de la distribucion prior Gamma(1, 0.25)",
        digits = 2,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")

```

La distribución a priori de $\lambda$ es asimétrica y sesgada hacia la derecha, reflejando que creemos que valores pequeños de goles por partido son más probables, aunque no descartamos valores mayores. La media de la prior es 4, la varianza es 16, lo que indica bastante incertidumbre sobre $\lambda$, y la probabilidad más alta a priori se concentra cerca de valores bajos.

### 2. ¿Por qué es razonable usar un modelo Poisson para nuestros datos $Y_i$?

Cada $Y_i$ representa el número de goles anotados en un partido, es decir, un conteo de eventos discretos que ocurren de manera independiente durante el encuentro. La distribución Poisson es adecuada porque modela eventos que suceden a una tasa promedio constante ($\lambda$) dentro de un intervalo fijo, en este caso, la duración del partido, y además no requiere un límite máximo de goles, ya que puede tomar cualquier valor entero no negativo.

### 3. El conjunto de datos `wwc_2019_matches` del paquete `fivethirtyeight` incluye el número de goles anotados por los dos equipos en cada partido de la Copa Mundial Femenina 2019. Defina, grafique y discuta el número total de goles por partido:

```{r warning=FALSE, message=FALSE}
library(fivethirtyeight)
data("wwc_2019_matches")
wwc_2019_matches <- wwc_2019_matches %>% 
  mutate(total_goals = score1 + score2)
```

```{r echo=FALSE, fig.width=6, fig.height=3}
ggplot(wwc_2019_matches, aes(x = total_goals)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  labs(title = "Distribucion del numero total de goles por partido",
       x = "Total de goles",
       y = "Cantidad de partidos") +
  theme_minimal()

# summary(wwc_2019_matches$total_goals)

tabla_resumen <- data.frame(
  Estadistico = c("Minimo", "1er Cuartil", "Mediana", "Media", "3er Cuartil", "Maximo"),
  Total_Goles = c(0, 2, 3, 2.808, 3, 13)
)

tabla_resumen %>%
  kable(caption = "Resumen del total de goles por partido – Copa Mundial Femenina 2019",
        digits = 2,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")
```

Al sumar los goles de ambos equipos, definimos el total de goles por partido (total_goals). La distribución de estos totales, visualizada mediante un histograma, muestra que la mayoría de los partidos tuvieron entre 2 y 5 goles, con una mediana de 3 y un promedio de aproximadamente 2.81 goles por partido. Aunque hay algunos partidos con valores extremos (hasta 13 goles), la distribución es ligeramente sesgada a la derecha, indicando que los partidos con muchos goles son menos frecuentes. Esta información sugiere que modelar los goles por partido con un enfoque de conteo discreto, como la distribución Poisson y el $\lambda$ que elegimos, es razonable.

\newpage

### 4. Identifique el modelo posterior de $\lambda$ y verifique su respuesta usando `summarize_gamma_poisson()`.


```{r echo = FALSE}
# Suma de goles observados
y <- sum(wwc_2019_matches$total_goals)
# Número de partidos
n <- nrow(wwc_2019_matches)

# Resumen del posterior Gamma-Poisson
posterior <- summarize_gamma_poisson(shape = 1, rate = 0.25, sum_y = y, n = n)
```
```{r echo = FALSE}
tabla_lambda <- data.frame(
  Modelo = c("Priori", "Posterior"),
  Shape  = c(1, 147),
  Rate   = c(0.25, 52.25),
  Media  = c(4.0000, 2.8134),
  Moda   = c(0.0000, 2.7943),
  Varianza = c(16.0000, 0.0538),
  Desviacion = c(4.0000, 0.2320)
)

tabla_lambda %>%
  kable(caption = "Resumen de la distribución prior y posterior de lambda",
        digits = 3,
        align = "c") %>%
  kable_styling(full_width = FALSE, position = "center")
```

### 5. Grafique la función de densidad priori, la función de verosimilitud (likelihood) y la densidad posterior de $\lambda$. Describa cómo evoluciona nuestro conocimiento de $\lambda$ desde la prior hasta la posterior.

```{r echo = FALSE, fig.width=7, fig.height=4}
plot_gamma_poisson(shape = 1, rate = 0.25, sum_y = y, n = n)
```
El gráfico muestra la evolución de nuestra creencia sobre $\lambda$, el promedio de 
goles por partido. La prior Gamma(1, 0.25) es fuertemente sesgada hacia la derecha, reflejando que inicialmente creíamos que valores pequeños de $\lambda$ eran más probables, aunque permitíamos valores grandes. En la posterior la varianza disminuye notablemente. En conjunto con la likelihood, esto muestra cómo los datos recientes ajustan y reducen la incertidumbre de nuestra estimación de $\lambda$.


# Ejercicio 5.12 (Cerebros de control)

Sea $\mu$ el volumen medio del hipocampo entre personas que no han sido diagnosticadas con una conmoción.  
Analizaremos \(\mu\) con el siguiente **modelo Normal–Normal**, donde los datos \(Y\) representan los volúmenes hipocampales de los individuos de este grupo:

\[
Y \mid \mu \stackrel{ind}{\sim} \text{Normal}(\mu,\, \sigma = 0.5), 
\qquad 
\mu \sim \text{Normal}(\theta = 6.5,\, \tau = 0.4).
\]

---

## Tareas

1. Usar los datos `football` para calcular la **media muestral** del volumen hipocampal y el **tamaño muestral** de los sujetos de control que no han sido diagnosticados con una conmoción.

```{r include=FALSE}
data(football)
datos <- football
datos
```


```{r include=FALSE}
contr_subjects <- datos %>%
  filter(group == "control")
nrow(contr_subjects)

m_v_hip <- mean(contr_subjects$volume)
m_v_hip
```
El grupo de control está compuesto por \(n = 25\) sujetos, con un volumen hipocampal medio de \(\bar{y} = 7.6\) centímetros cúbicos.


2. Identificar el **modelo posterior** de \(\mu\) y verificar la respuesta con `summarize_normal_normal()`.
## Paso 2: Modelo posterior

La distribución posterior de \(\mu\) bajo el modelo Normal–Normal es:

\[
\mu \mid y \sim \text{Normal}\!\Bigg(
\frac{\tfrac{\theta}{\tau^2} + \tfrac{n \bar{y}}{\sigma^2}}
     {\tfrac{1}{\tau^2} + \tfrac{n}{\sigma^2}},
\quad
\frac{1}{\tfrac{1}{\tau^2} + \tfrac{n}{\sigma^2}}
\Bigg)
\]

donde \(\bar{y} = 7.6\), \(n = 25\), \(\sigma = 0.5\), \(\theta = 6.5\) y \(\tau = 0.4\).

```{r}
summarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5,
                        y_bar = 7.6, n = 25)
```

3. Graficar la **pdf previa**, la **función de verosimilitud** y la **pdf posterior** de \(\mu\).
```{r echo = FALSE, fig.width=4, fig.height=3}
plot_normal_likelihood(y = contr_subjects$volume, sigma = 0.5)
```

```{r echo = FALSE, fig.width=4, fig.height=3}
plot_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5,
                        y_bar = 7.6, n = 25)
```

4. Describir cómo evoluciona la comprensión de \(\mu\) desde la previa hasta la posterior. 

## Paso 4: Evolución de la comprensión de \(\mu\)

En la **distribución previa**, nuestro conocimiento sobre \(\mu\) estaba centrado en 
\(\theta = 6.5\) con bastante dispersión (\(\tau = 0.4\)).  
La curva amarilla refleja esta creencia inicial, amplia e incierta.  

La **verosimilitud** (curva azul) incorpora la información de los datos: 
la media muestral observada en el grupo de control fue \(\bar{y} = 7.6\) con 
\(n = 25\) y \(\sigma = 0.5\).  
Esta curva está mucho más concentrada y centrada cerca de 7.6, 
lo que muestra que los datos son bastante informativos.  

La **distribución posterior** (curva verde) combina la previa con los datos.  
Se observa cómo se desplaza hacia valores cercanos a 7.5 
y se concentra fuertemente en torno a ese punto 
(\(\text{sd posterior} \approx 0.097\)).  

En conclusión, los datos actualizan de forma sustancial nuestra comprensión:  
pasamos de una creencia amplia y centrada en 6.5 a una estimación más precisa, 
muy cercana a la media observada en la muestra de control.


Ejercicio Normal-Normal (guiado con el libro):

(i) Calcular la distribución a posteriori para $μ$. 

(ii) Demostrar que la distribución a posteriori para μ es normal con media igual a un promedio ponderado de la media a priori $θ$ y de la media muestral.

(iii) Analizar la influencia del tamaño muestral $n$ sobre la media y la varianza de la distribución a posterior para $μ$. Conclusión: la distribución a priori normal para $μ$ es conjugada con el modelo muestral de observaciones independientes de una población normal con media $μ$ y varianza conocida $σ^2$


# Ejercicio Normal–Normal (guiado con el libro)

Sea \(\mu\) el volumen medio del hipocampo. Supondremos el **modelo Normal–Normal** con varianza conocida \(\sigma^2\):

\[
Y_i \mid \mu \stackrel{ind}{\sim} \mathcal{N}(\mu,\sigma^2), \quad i=1,\dots,n,
\qquad
\mu \sim \mathcal{N}(\theta,\tau^2).
\]

Denotemos \(\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i\).

---

## (i) Distribución a posteriori de \(\mu\)

La distribución a posteriori es también Normal:

\[
\mu \mid \mathbf{y} \sim \mathcal{N}\!\Big(\, m_n,\; v_n \Big),
\]
con
\[
v_n \;=\; \frac{1}{\tfrac{1}{\tau^2}+\tfrac{n}{\sigma^2}},
\qquad
m_n \;=\; v_n\!\left(\frac{\theta}{\tau^2}+\frac{n\,\bar{y}}{\sigma^2}\right).
\]

---

## (ii) Media posterior como **promedio ponderado** de \(\theta\) y \(\bar{y}\)

Escribiendo los pesos explícitamente:

\[
m_n \;=\; 
\underbrace{\left(\frac{1/\tau^2}{\,1/\tau^2+n/\sigma^2\,}\right)}_{\text{peso previo}}
\theta
\;+\;
\underbrace{\left(\frac{(n/\sigma^2)}{\,1/\tau^2+n/\sigma^2\,}\right)}_{\text{peso de los datos}}
\bar{y}.
\]

Equivalente, en términos de **precisiones** (inversas de varianzas):

\[
m_n \;=\; 
\frac{\text{precisión previa}\cdot \theta + \text{precisión de la media muestral}\cdot \bar{y}}
{\text{precisión previa}+\text{precisión de la media muestral}},
\quad
\text{donde }
\text{precisión previa}=1/\tau^2,\;\;
\text{precisión de la media muestral}=n/\sigma^2.
\]

---

## (iii) Influencia de \(n\) en la media y varianza a posteriori

- **Varianza posterior**:
  \[
  v_n=\frac{1}{1/\tau^2+n/\sigma^2}
  \quad\Rightarrow\quad
  v_n \downarrow \text{ al crecer } n.
  \]
  Más datos \(\Rightarrow\) **más precisión** (menor incertidumbre).

- **Media posterior**:
  \[
  m_n
  = \Bigg(\frac{1/\tau^2}{1/\tau^2+n/\sigma^2}\Bigg)\theta
  + \Bigg(\frac{n/\sigma^2}{1/\tau^2+n/\sigma^2}\Bigg)\bar{y}.
  \]
  Al aumentar \(n\), el **peso de los datos** \(\frac{n/\sigma^2}{1/\tau^2+n/\sigma^2}\) crece y 
  \(m_n\) se acerca a \(\bar{y}\).
  - Límite \(n\to\infty\): \(m_n \to \bar{y}\), \(v_n \to \sigma^2/n \to 0\).
  - Límite \(n\to 0\): \(m_n \to \theta\), \(v_n \to \tau^2\).

---

## Conclusión (conjugación)

La **Normal** es conjugada para la media \(\mu\) bajo un modelo Normal con \(\sigma^2\) conocida: una previa \(\mathcal{N}(\theta,\tau^2)\) produce una posterior \(\mathcal{N}(m_n,v_n)\) con:
\[
v_n=\frac{1}{1/\tau^2+n/\sigma^2}, 
\qquad
m_n=\frac{\tfrac{\theta}{\tau^2}+\tfrac{n\bar{y}}{\sigma^2}}{\tfrac{1}{\tau^2}+\tfrac{n}{\sigma^2}}.
\]
La media posterior es un **promedio ponderado** entre la creencia previa \(\theta\) y la evidencia \(\bar{y}\), con pesos proporcionales a sus **precisiones** respectivas.










